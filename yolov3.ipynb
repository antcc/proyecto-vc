{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"widerface.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7rt6uoknw5hu","colab_type":"code","outputId":"429389ca-7d32-492b-9ce8-ad49f1214c62","executionInfo":{"status":"ok","timestamp":1578230017620,"user_tz":-60,"elapsed":27444,"user":{"displayName":"Antonio CC","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mALMYvQuCKdoZ162E0MXUsg4q6JR6WX3laNsxuz=s64","userId":"15294922141235348228"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["%tensorflow_version 1.x\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount = True)\n","\n","# Directorio de trabajo\n","DIR = \"/content/drive/My Drive/vc/\" "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vmH8M3ZZzGoD","colab_type":"code","colab":{}},"source":["import sys \n","import os\n","sys.path.append(os.path.abspath(DIR))\n","\n","import numpy as np\n","import json\n","from voc import parse_voc_annotation\n","from yolo import create_yolov3_model, dummy_loss\n","from generator import BatchGenerator\n","from utils.utils import normalize, evaluate, makedirs\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from keras.optimizers import Adam\n","from callbacks import CustomModelCheckpoint, CustomTensorBoard\n","from utils.multi_gpu_model import multi_gpu_model\n","import tensorflow as tf\n","import keras\n","from keras.models import load_model\n","\n","EPOCHS = 55 \n","WARMUP_EPOCHS = 5\n","\n","def create_training_instances(\n","    train_annot_folder,\n","    train_image_folder,\n","    train_cache,\n","    valid_annot_folder,\n","    valid_image_folder,\n","    valid_cache,\n","    labels,\n","):\n","    # parse annotations of the training set\n","    train_ints, train_labels = parse_voc_annotation(train_annot_folder, train_image_folder, train_cache, labels)\n","\n","    # parse annotations of the validation set, if any, otherwise split the training set\n","    if os.path.exists(valid_annot_folder):\n","        valid_ints, valid_labels = parse_voc_annotation(valid_annot_folder, valid_image_folder, valid_cache, labels)\n","    else:\n","        print(\"valid_annot_folder not exists. Spliting the trainining set.\")\n","\n","        train_valid_split = int(0.8*len(train_ints))\n","        np.random.seed(0)\n","        np.random.shuffle(train_ints)\n","        np.random.seed()\n","\n","        valid_ints = train_ints[train_valid_split:]\n","        train_ints = train_ints[:train_valid_split]\n","\n","    # compare the seen labels with the given labels in config.json\n","    if len(labels) > 0:\n","        overlap_labels = set(labels).intersection(set(train_labels.keys()))\n","\n","        print('Seen labels: \\t'  + str(train_labels) + '\\n')\n","        print('Given labels: \\t' + str(labels))\n","\n","        # return None, None, None if some given label is not in the dataset\n","        if len(overlap_labels) < len(labels):\n","            print('Some labels have no annotations! Please revise the list of labels in the config.json.')\n","            return None, None, None\n","    else:\n","        print('No labels are provided. Train on all seen labels.')\n","        print(train_labels)\n","        labels = train_labels.keys()\n","\n","    max_box_per_image = max([len(inst['object']) for inst in (train_ints + valid_ints)])\n","\n","    return train_ints, valid_ints, sorted(labels), max_box_per_image\n","\n","def create_callbacks(saved_weights_name, tensorboard_logs, model_to_save):\n","    makedirs(tensorboard_logs)\n","\n","    early_stop = EarlyStopping(\n","        monitor     = 'loss',\n","        min_delta   = 0.01,\n","        patience    = 5,\n","        mode        = 'min',\n","        verbose     = 1\n","    )\n","    checkpoint = CustomModelCheckpoint(\n","        model_to_save   = model_to_save,\n","        filepath        = saved_weights_name,# + '{epoch:02d}.h5',\n","        monitor         = 'loss',\n","        verbose         = 1,\n","        save_best_only  = True,\n","        mode            = 'min',\n","        period          = 1\n","    )\n","    reduce_on_plateau = ReduceLROnPlateau(\n","        monitor  = 'loss',\n","        factor   = 0.1,\n","        patience = 2,\n","        verbose  = 1,\n","        mode     = 'min',\n","        epsilon  = 0.01,\n","        cooldown = 0,\n","        min_lr   = 0\n","    )\n","    tensorboard = CustomTensorBoard(\n","        log_dir                = tensorboard_logs,\n","        write_graph            = True,\n","        write_images           = True,\n","    )\n","    return [early_stop, checkpoint, reduce_on_plateau, tensorboard]\n","\n","def create_model(\n","    nb_class,\n","    anchors,\n","    max_box_per_image,\n","    max_grid, batch_size,\n","    warmup_batches,\n","    ignore_thresh,\n","    multi_gpu,\n","    saved_weights_name,\n","    lr,\n","    grid_scales,\n","    obj_scale,\n","    noobj_scale,\n","    xywh_scale,\n","    class_scale\n","):\n","    if multi_gpu > 1:\n","        with tf.device('/cpu:0'):\n","            template_model, infer_model = create_yolov3_model(\n","                nb_class            = nb_class,\n","                anchors             = anchors,\n","                max_box_per_image   = max_box_per_image,\n","                max_grid            = max_grid,\n","                batch_size          = batch_size//multi_gpu,\n","                warmup_batches      = warmup_batches,\n","                ignore_thresh       = ignore_thresh,\n","                grid_scales         = grid_scales,\n","                obj_scale           = obj_scale,\n","                noobj_scale         = noobj_scale,\n","                xywh_scale          = xywh_scale,\n","                class_scale         = class_scale\n","            )\n","    else:\n","        template_model, infer_model = create_yolov3_model(\n","            nb_class            = nb_class,\n","            anchors             = anchors,\n","            max_box_per_image   = max_box_per_image,\n","            max_grid            = max_grid,\n","            batch_size          = batch_size,\n","            warmup_batches      = warmup_batches,\n","            ignore_thresh       = ignore_thresh,\n","            grid_scales         = grid_scales,\n","            obj_scale           = obj_scale,\n","            noobj_scale         = noobj_scale,\n","            xywh_scale          = xywh_scale,\n","            class_scale         = class_scale\n","        )\n","\n","    # load the pretrained weight if exists, otherwise load the backend weight only\n","    if os.path.exists(saved_weights_name):\n","        print(\"\\nLoading pretrained weights.\\n\")\n","        template_model.load_weights(saved_weights_name)\n","    else:\n","        template_model.load_weights(DIR + \"backend.h5\", by_name=True)\n","\n","    if multi_gpu > 1:\n","        train_model = multi_gpu_model(template_model, gpus=multi_gpu)\n","    else:\n","        train_model = template_model\n","\n","    optimizer = Adam(lr=lr, clipnorm=0.001)\n","    train_model.compile(loss=dummy_loss, optimizer=optimizer)\n","\n","    return train_model, infer_model\n","\n","def _main_():\n","    config_path = DIR + \"config.json\"\n","\n","    with open(config_path) as config_buffer:\n","        config = json.loads(config_buffer.read())\n","\n","    ###############################\n","    #   Parse the annotations\n","    ###############################\n","    train_ints, valid_ints, labels, max_box_per_image = create_training_instances(\n","        config['train']['train_annot_folder'],\n","        config['train']['train_image_folder'],\n","        config['train']['cache_name'],\n","        config['valid']['valid_annot_folder'],\n","        config['valid']['valid_image_folder'],\n","        config['valid']['cache_name'],\n","        config['model']['labels']\n","    )\n","    print('\\nTraining on: \\t' + str(labels) + '\\n')\n","\n","    ###############################\n","    #   Create the generators\n","    ###############################\n","    train_generator = BatchGenerator(\n","        instances           = train_ints,\n","        anchors             = config['model']['anchors'],\n","        labels              = labels,\n","        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n","        max_box_per_image   = max_box_per_image,\n","        batch_size          = config['train']['batch_size'],\n","        min_net_size        = config['model']['min_input_size'],\n","        max_net_size        = config['model']['max_input_size'],\n","        shuffle             = True,\n","        jitter              = 0.3,\n","        norm                = normalize\n","    )\n","\n","    valid_generator = BatchGenerator(\n","        instances           = valid_ints,\n","        anchors             = config['model']['anchors'],\n","        labels              = labels,\n","        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n","        max_box_per_image   = max_box_per_image,\n","        batch_size          = config['train']['batch_size'],\n","        min_net_size        = config['model']['min_input_size'],\n","        max_net_size        = config['model']['max_input_size'],\n","        shuffle             = True,\n","        jitter              = 0.0,\n","        norm                = normalize\n","    )\n","\n","    ###############################\n","    #   Create the model\n","    ###############################\n","    config['train']['warmup_epochs'] = WARMUP_EPOCHS\n","    warmup_batches = config['train']['warmup_epochs'] * (config['train']['train_times']*len(train_generator))\n","\n","    os.environ['CUDA_VISIBLE_DEVICES'] = config['train']['gpus']\n","    multi_gpu = len(config['train']['gpus'].split(','))\n","\n","    train_model, infer_model = create_model(\n","        nb_class            = len(labels),\n","        anchors             = config['model']['anchors'],\n","        max_box_per_image   = max_box_per_image,\n","        max_grid            = [config['model']['max_input_size'], config['model']['max_input_size']],\n","        batch_size          = config['train']['batch_size'],\n","        warmup_batches      = warmup_batches,\n","        ignore_thresh       = config['train']['ignore_thresh'],\n","        multi_gpu           = multi_gpu,\n","        saved_weights_name  = config['train']['saved_weights_name'],\n","        lr                  = config['train']['learning_rate'],\n","        grid_scales         = config['train']['grid_scales'],\n","        obj_scale           = config['train']['obj_scale'],\n","        noobj_scale         = config['train']['noobj_scale'],\n","        xywh_scale          = config['train']['xywh_scale'],\n","        class_scale         = config['train']['class_scale'],\n","    )\n","\n","    ###############################\n","    #   Kick off the training\n","    ###############################\n","    callbacks = create_callbacks(config['train']['saved_weights_name'], config['train']['tensorboard_dir'], infer_model)\n","\n","    train_model.fit_generator(\n","        generator        = train_generator,\n","        steps_per_epoch  = len(train_generator) * config['train']['train_times'],\n","        epochs           = EPOCHS + config['train']['warmup_epochs'],\n","        verbose          = 2 if config['train']['debug'] else 1,\n","        callbacks        = callbacks,\n","        workers          = 4,\n","        max_queue_size   = 8\n","    )\n","\n","\n","    infer_model = load_model(config['train']['saved_weights_name'])\n","\n","    ###############################\n","    #   Run the evaluation\n","    ###############################\n","    # compute mAP for all the classes\n","    print(\"\\nEvaluating model on validation set...\\n\")\n","    average_precisions = evaluate(infer_model, valid_generator)\n","\n","    # print the score\n","    for label, average_precision in average_precisions.items():\n","        print(labels[label] + ': {:.4f}'.format(average_precision))\n","    print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nBV0We-91w3r","colab_type":"code","outputId":"bd9f13be-f5e0-4b7a-bd5f-9ed26dd7b1f9","executionInfo":{"status":"ok","timestamp":1578231368692,"user_tz":-60,"elapsed":1317312,"user":{"displayName":"Antonio CC","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mALMYvQuCKdoZ162E0MXUsg4q6JR6WX3laNsxuz=s64","userId":"15294922141235348228"}},"colab":{"base_uri":"https://localhost:8080/","height":627}},"source":["config_path = DIR + \"config.json\"\n","with open(config_path) as config_buffer:\n","    config = json.loads(config_buffer.read())\n","\n","train_ints, valid_ints, labels, max_box_per_image = create_training_instances(\n","    config['train']['train_annot_folder'],\n","    config['train']['train_image_folder'],\n","    config['train']['cache_name'],\n","    config['valid']['valid_annot_folder'],\n","    config['valid']['valid_image_folder'],\n","    config['valid']['cache_name'],\n","    config['model']['labels']\n",")\n","\n","valid_generator = BatchGenerator(\n","        instances           = valid_ints,\n","        anchors             = config['model']['anchors'],\n","        labels              = labels,\n","        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n","        max_box_per_image   = max_box_per_image,\n","        batch_size          = config['train']['batch_size'],\n","        min_net_size        = config['model']['min_input_size'],\n","        max_net_size        = config['model']['max_input_size'],\n","        shuffle             = True,\n","        jitter              = 0.0,\n","        norm                = normalize\n","    )\n","\n","infer_model = load_model(config['train']['saved_weights_name'])\n","\n","# compute mAP for all the classes\n","print(\"\\nEvaluating model on validation set...\\n\")\n","average_precisions = evaluate(infer_model, valid_generator)\n","\n","# print the score\n","for label, average_precision in average_precisions.items():\n","    print(labels[label] + ': {:.4f}'.format(average_precision))\n","print('mAP: {:.4f}'.format(sum(average_precisions.values()) / len(average_precisions)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Seen labels: \t{'face': 159424}\n","\n","Given labels: \t['face']\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n","  warnings.warn('No training configuration found in save file: '\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Evaluating model on validation set...\n","\n","face: 0.4739\n","mAP: 0.4739\n"],"name":"stdout"}]}]}