{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5MpWcIZKRia",
        "colab_type": "text"
      },
      "source": [
        "###### *Fuente: https://github.com/antcc/proyecto-vc*\n",
        "# Detección de caras con YOLOv3\n",
        "Utilizaremos la base de datos [WIDERFACE](http://shuoyang1213.me/WIDERFACE/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQGi1r78Kk5K",
        "colab_type": "text"
      },
      "source": [
        "## Preliminares\n",
        "Montamos nuestro *drive* y establecemos el directorio de trabajo. Debemos tener los siguientes archivos disponibles en el mismo:\n",
        "\n",
        "* Una carpeta **yolo** con los ficheros de código disponibles en el repositorio.\n",
        "* Una carpeta llamada **data** que contenga a su vez 4 subcarpetas:\n",
        "  * **train**: contiene las imágenes de entrenamiento.\n",
        "  * **train_annot**: contiene las anotaciones de las imágenes de entrenamiento en formato VOC.\n",
        "  * **valid**: contiene las imágenes de validación.\n",
        "  * **valid_annot**: contiene las anotaciones de las imágenes de validación en formato VOC.\n",
        "* Un archivo *config.json* que defina el entorno de configuración, similar al disponible en el respositorio.\n",
        "* Una carpeta **models** donde se guardarán los modelos entrenados. Debe contener inicialmente un archivo *backend.h5* con pesos preentrenados en la red concreta que se ha implementado. Por ejemplo, unos pesos preentrenados en la base de datos COCO pueden obtenerse [aquí](https://s3-ap-southeast-1.amazonaws.com/deeplearning-mat/backend.h5).\n",
        "\n",
        "Durante el entrenamiento se crearán automáticamente archivos auxiliares en el directorio de trabajo.\n",
        "\n",
        "*Nota:* las anotaciones en formato VOC pueden obtenerse a partir de las originales empleando el script `yolo/utils/convert_annot.py`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rt6uoknw5hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import sys \n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Directorio de trabajo\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "DIR = \"/content/drive/My Drive/vc/\" \n",
        "sys.path.append(os.path.abspath(DIR))\n",
        "model_loaded = False\n",
        "\n",
        "# Archivo de configuración\n",
        "config_path = DIR + \"config.json\"\n",
        "with open(config_path) as config_buffer:\n",
        "    config = json.loads(config_buffer.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLi-7ocKK1Rj",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento\n",
        "Creamos un modelo de la red YOLOv3, que entrenamos con los datos de entrenamiento. En el archivo de configuración, los parámetros de entrenamiento más destacables son:\n",
        "\n",
        "* *min_input_size* y *max_input_size*: controlan el rango en el que serán redimensionadas las imágenes de entrenamiento. Deben ser múltiplos de 32.\n",
        "* *input_size*: controla el tamaño de entrada de las imágenes de validación. Debe ser múltiplo de 32.\n",
        "* *batch_size* y *learning_rate*: Tamaño de cada *batch* y *learning rate* inicial del optimizador Adam.\n",
        "* *nb_epochs* y *warmup_epochs*: establecen el número de épocas de entrenamiento normales y el número de épocas de entrenamiento iniciales en las que se fuerza a que las *boxes* coincidan con los *anchors*. En total se entranará durante un número de épocas igual a la suma de estos parámetros.\n",
        "* *saved_weights_name*: indica la ruta en la que se guardará el modelo entrenado en formato HDF5.\n",
        "\n",
        "Disponemos de un *callback* de *early stopping* que detiene el entrenamiento si la función de pérdida no mejora en 10 épocas. También tenemos otro *callback* que reduce el *learning rate* en un orden de magnitud si la pérdida no mejora en 2 épocas.\n",
        "\n",
        "El archivo con el modelo entrenado puede usarse para renaudar el entrenamiento (cargándolo con `load_weights`) o para realizar predicciones (cargándolo con `load_model`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmH8M3ZZzGoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from keras.models import load_model\n",
        "from yolo.train import create_training_instances, create_model, create_callbacks\n",
        "from yolo.generator import BatchGenerator\n",
        "from yolo.utils.utils import normalize\n",
        "\n",
        "def _train(epochs):\n",
        "    ###############################\n",
        "    #   Leemos anotaciones\n",
        "    ###############################\n",
        "    train_ints, _, labels, max_box_per_image = create_training_instances(\n",
        "        config['train']['train_annot_folder'],\n",
        "        config['train']['train_image_folder'],\n",
        "        config['train']['cache_name'],\n",
        "        config['valid']['valid_annot_folder'],\n",
        "        config['valid']['valid_image_folder'],\n",
        "        config['valid']['cache_name'],\n",
        "        config['model']['labels']\n",
        "    )\n",
        "    print('\\nTraining on: \\t' + str(labels) + '\\n')\n",
        "\n",
        "    ##################################\n",
        "    #   Creamos generador de imágenes\n",
        "    ##################################\n",
        "    train_generator = BatchGenerator(\n",
        "        instances           = train_ints,\n",
        "        anchors             = config['model']['anchors'],\n",
        "        labels              = labels,\n",
        "        downsample          = 32, # ratio between network input's size and network output's size, 32 for YOLOv3\n",
        "        max_box_per_image   = max_box_per_image,\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        min_net_size        = config['model']['min_input_size'],\n",
        "        max_net_size        = config['model']['max_input_size'],\n",
        "        shuffle             = True,\n",
        "        jitter              = 0.3,\n",
        "        norm                = normalize\n",
        "    )\n",
        "\n",
        "    ###############################\n",
        "    #   Creamos el modelo\n",
        "    ###############################\n",
        "    if os.path.exists(config['train']['saved_weights_name']):\n",
        "        config['train']['warmup_epochs'] = 0\n",
        "    warmup_batches = config['train']['warmup_epochs'] * (config['train']['train_times']*len(train_generator))\n",
        "\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = config['train']['gpus']\n",
        "    multi_gpu = len(config['train']['gpus'].split(','))\n",
        "\n",
        "    train_model, infer_model = create_model(\n",
        "        nb_class            = len(labels),\n",
        "        anchors             = config['model']['anchors'],\n",
        "        max_box_per_image   = max_box_per_image,\n",
        "        max_grid            = [config['model']['max_input_size'], config['model']['max_input_size']],\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        warmup_batches      = warmup_batches,\n",
        "        ignore_thresh       = config['train']['ignore_thresh'],\n",
        "        multi_gpu           = multi_gpu,\n",
        "        saved_weights_name  = config['train']['saved_weights_name'],\n",
        "        lr                  = config['train']['learning_rate'],\n",
        "        grid_scales         = config['train']['grid_scales'],\n",
        "        obj_scale           = config['train']['obj_scale'],\n",
        "        noobj_scale         = config['train']['noobj_scale'],\n",
        "        xywh_scale          = config['train']['xywh_scale'],\n",
        "        class_scale         = config['train']['class_scale'],\n",
        "        backend_path        = DIR + \"models/backend.h5\"\n",
        "    )\n",
        "\n",
        "    ################################\n",
        "    #   Comenzamos el entrenamiento\n",
        "    ################################\n",
        "    callbacks = create_callbacks(config['train']['saved_weights_name'], config['train']['tensorboard_dir'], infer_model)\n",
        "\n",
        "    hist = train_model.fit_generator(\n",
        "        generator        = train_generator,\n",
        "        steps_per_epoch  = len(train_generator) * config['train']['train_times'],\n",
        "        epochs           = epochs + config['train']['warmup_epochs'],\n",
        "        verbose          = 2 if config['train']['debug'] else 1,\n",
        "        callbacks        = callbacks,\n",
        "        workers          = 4,\n",
        "        max_queue_size   = 8\n",
        "    )\n",
        "\n",
        "    model_loaded = False\n",
        "\n",
        "    print(\"\\nTraining completed. Saved training history.\")\n",
        "    with open(config['train']['tensorboard_dir'] + \"train_hist\", 'wb') as f:\n",
        "        pickle.dump(hist, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtaPXkCWi89",
        "colab_type": "text"
      },
      "source": [
        "## Evaluación\n",
        "\n",
        "Evaluamos el modelo obtenido en el conjunto de validación. La métrica de evaluación utilizada es la precisión media o *mAP* tal y como se evalúa en el *COCO Challenge* desde 2017. La media se toma variando en 10 umbrales distintos de IoU: 0.5, 0.6, 0.65, ..., 0.95. Tras finalizar, se guardan las *bounding boxes* detectadas para cada imagen en el archivo definido en la variable *valid_result* del archivo de configuración. Se utiliza el formato definido [aquí](https://competitions.codalab.org/competitions/20146#learn_the_details-overview).\n",
        "\n",
        "También se proporciona una función para detectar caras en una imagen concreta, mostrando gráficamente el resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBV0We-91w3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yolo.evaluate import evaluate_coco\n",
        "from yolo.utils.bbox import draw_boxes\n",
        "from yolo.utils.utils import get_yolo_boxes\n",
        "from yolo.generator import BatchGenerator\n",
        "from yolo.train import create_training_instances\n",
        "from yolo.utils.utils import normalize\n",
        "from keras.models import load_model\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _evaluate(model):\n",
        "    # Leemos las anotaciones\n",
        "    _, valid_ints, labels, max_box_per_image = create_training_instances(\n",
        "        config['train']['train_annot_folder'],\n",
        "        config['train']['train_image_folder'],\n",
        "        config['train']['cache_name'],\n",
        "        config['valid']['valid_annot_folder'],\n",
        "        config['valid']['valid_image_folder'],\n",
        "        config['valid']['cache_name'],\n",
        "        config['model']['labels']\n",
        "    )\n",
        "\n",
        "    # Creamos el generador de imágenes de validación\n",
        "    valid_generator = BatchGenerator(\n",
        "        instances           = valid_ints,\n",
        "        anchors             = config['model']['anchors'],\n",
        "        labels              = labels,\n",
        "        downsample          = 32,\n",
        "        max_box_per_image   = max_box_per_image,\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        min_net_size        = config['model']['min_input_size'],\n",
        "        max_net_size        = config['model']['max_input_size'],\n",
        "        shuffle             = True,\n",
        "        jitter              = 0.0,\n",
        "        norm                = normalize\n",
        "    )\n",
        "\n",
        "    # Calculamos la métrica mAP y guardamos los resultados en fichero\n",
        "    average_precision = evaluate_coco(model,\n",
        "                                      valid_generator,\n",
        "                                      obj_thresh=0.5,\n",
        "                                      nms_thresh=0.5,\n",
        "                                      net_h=config['model']['input_size'],\n",
        "                                      net_w=config['model']['input_size'],\n",
        "                                      save_path=config['valid']['valid_result'])[0]\n",
        "\n",
        "    # Imprimimos el resultado\n",
        "    print('mAP: {:.4f}'.format(average_precision))\n",
        "\n",
        "def _detect(model, filename):\n",
        "    # Leemos la imagen\n",
        "    im = cv2.cvtColor(cv2.imread(filename, 1), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Predecimos las bounding boxes     \n",
        "    pred_boxes = get_yolo_boxes(model, \n",
        "                                images = [im],\n",
        "                                net_h = config['model']['input_size'], \n",
        "                                net_w = config['model']['input_size'], \n",
        "                                anchors = config['model']['anchors'],\n",
        "                                obj_thresh = 0.5, \n",
        "                                nms_thresh = 0.5)[0]\n",
        "  \n",
        "    # Mostramos la imagen con los resultados\n",
        "    im_boxes = draw_boxes(im, pred_boxes, config['model']['labels'], obj_thresh = 0.5)\n",
        "    plt.figure(figsize = (8, 8))\n",
        "    plt.imshow(im_boxes)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36PHlVlCROP5",
        "colab_type": "text"
      },
      "source": [
        "## Prueba de funcionamiento\n",
        "\n",
        "Probamos ahora a entrenar y evaluar el modelo. También pintamos una imagen del conjunto de validación para visualizar las caras detectadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e-ym4bZWgwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos las épocas de entrenamiento\n",
        "AUTO = False\n",
        "EPOCHS = 58\n",
        "epochs = config['train']['nb_epochs'] if AUTO else EPOCHS\n",
        "\n",
        "# Entrenamos el modelo\n",
        "_train(epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqFMlnx1bUK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cargamos el modelo\n",
        "if not model_loaded:\n",
        "    yolov3 = load_model(config['train']['saved_weights_name'])\n",
        "    model_loaded = True\n",
        "\n",
        "# Evaluamos el modelo\n",
        "_evaluate(yolov3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6mIJnZbcBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cargamos el modelo\n",
        "if not model_loaded:\n",
        "    yolov3 = load_model(config['train']['saved_weights_name'])\n",
        "    model_loaded = True\n",
        "\n",
        "# Definimos la imagen de prueba\n",
        "filename = DIR + \"data/valid/9--Press_Conference/9_Press_Conference_Press_Conference_9_74.jpg\"\n",
        "\n",
        "# Detectamos caras en la imagen de prueba\n",
        "_detect(yolov3, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}