{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antcc/proyecto-vc/blob/master/yolov3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5MpWcIZKRia",
        "colab_type": "text"
      },
      "source": [
        "###### *Fuente: https://github.com/antcc/proyecto-vc*\n",
        "# Detección de caras con YOLOv3\n",
        "Utilizaremos la base de datos [WIDERFACE](http://shuoyang1213.me/WIDERFACE/), que consta de 12881 imágenes de entrenamiento y 3220 imágenes de validación, todas anotadas con los valores de *ground truth* de las *bounding boxes*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQGi1r78Kk5K",
        "colab_type": "text"
      },
      "source": [
        "## Preliminares\n",
        "Montamos nuestro *drive* y establecemos el directorio de trabajo. Debemos tener los siguientes archivos disponibles en el mismo:\n",
        "\n",
        "* Una carpeta **yolo** con los ficheros de código disponibles en el repositorio, respetando la estructura presente en el mismo.\n",
        "* Una carpeta llamada **widerface** que contenga a su vez 4 subcarpetas:\n",
        "  * **train**: contiene las imágenes de entrenamiento.\n",
        "  * **train_annot**: contiene las anotaciones de las imágenes de entrenamiento en formato VOC.\n",
        "  * **valid**: contiene las imágenes de validación.\n",
        "  * **valid_annot**: contiene las anotaciones de las imágenes de validación en formato VOC.\n",
        "* Un archivo *config.json* que defina el entorno de configuración, similar al disponible en el respositorio.\n",
        "* Una carpeta **models** donde se guardarán los modelos entrenados. Debe contener inicialmente un archivo *backend.h5* con pesos preentrenados en la red concreta que se ha implementado. Por ejemplo, unos pesos preentrenados en la base de datos COCO pueden obtenerse [aquí](https://s3-ap-southeast-1.amazonaws.com/deeplearning-mat/backend.h5).\n",
        "\n",
        "Durante el entrenamiento se crearán automáticamente archivos auxiliares en el directorio de trabajo.\n",
        "\n",
        "*Nota:* las anotaciones en formato VOC pueden obtenerse a partir de las originales empleando el script `yolo/utils/convert_annot.py`.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rt6uoknw5hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import sys \n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Directorio de trabajo\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "DIR = \"/content/drive/My Drive/vc/\" \n",
        "sys.path.append(os.path.abspath(DIR))\n",
        "model_loaded = False\n",
        "\n",
        "# Archivo de configuración\n",
        "config_path = DIR + \"config.json\"\n",
        "with open(config_path) as config_buffer:\n",
        "    config = json.loads(config_buffer.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUKvWtFX3v8f",
        "colab_type": "text"
      },
      "source": [
        "Definimos también una serie de funciones auxiliares para trabajar con imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctLnqCP831w9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def norm(im):\n",
        "    \"\"\"Normaliza una imagen de números reales a [0,1]\"\"\"\n",
        "\n",
        "    return cv2.normalize(im, None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
        "\n",
        "def read_im(filename, color_flag = 1):\n",
        "    \"\"\"Devuelve una imagen de números reales adecuadamente leída en grises o en color.\n",
        "        - filename: ruta de la imagen.\n",
        "        - color_flag: indica si es en color (1) o en grises (0).\"\"\"\n",
        "\n",
        "    try:\n",
        "        im = cv2.imread(filename, color_flag)\n",
        "        if len(im.shape) == 3:\n",
        "            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    except:\n",
        "        print(\"Error: no se ha podido cargar la imagen \" + filename)\n",
        "        quit()\n",
        "\n",
        "    return im.astype(np.double)\n",
        "\n",
        "def print_im(im, title = \"\", show = True, tam = (10, 10)):\n",
        "    \"\"\"Muestra una imagen cualquiera normalizada.\n",
        "        - im: imagen a mostrar.\n",
        "        - show: indica si queremos mostrar la imagen inmediatamente.\n",
        "        - tam = (width, height): tamaño del plot.\"\"\"\n",
        "\n",
        "    show_title = len(title) > 0\n",
        "\n",
        "    if show:\n",
        "        fig = plt.figure(figsize = tam)\n",
        "\n",
        "    im = norm(im)  # Normalizamos a [0,1]\n",
        "    plt.imshow(im, interpolation = None, cmap = 'gray')\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "\n",
        "    if show:\n",
        "        if show_title:\n",
        "            plt.title(title)\n",
        "        plt.show() \n",
        "\n",
        "def print_multiple_im(vim, titles = \"\", ncols = 2, tam = (10, 10)):\n",
        "    \"\"\"Muestra una sucesión de imágenes en la misma ventana, eventualmente con sus títulos.\n",
        "        - vim: sucesión de imágenes a mostrar.\n",
        "        - titles: o bien vacío o bien una sucesión de títulos del mismo tamaño que vim.\n",
        "        - ncols: número de columnas del multiplot.\n",
        "        - tam = (width, height): tamaño del multiplot.\"\"\"\n",
        "\n",
        "    show_title = len(titles) > 0\n",
        "\n",
        "    nrows = len(vim) // ncols + (0 if len(vim) % ncols == 0 else 1)\n",
        "    plt.figure(figsize = tam)\n",
        "\n",
        "    for i in range(len(vim)):\n",
        "        plt.subplot(nrows, ncols, i + 1)\n",
        "        if show_title:\n",
        "            plt.title(titles[i])\n",
        "        print_im(vim[i], title = \"\", show = False)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLi-7ocKK1Rj",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento\n",
        "Creamos un modelo de la red YOLOv3, que entrenamos con los datos de entrenamiento. En el archivo de configuración, los parámetros de entrenamiento más destacables son:\n",
        "\n",
        "* *min_input_size* y *max_input_size*: controlan el rango en el que serán redimensionadas las imágenes de entrenamiento. Deben ser múltiplos de 32.\n",
        "* *input_size*: controla el tamaño de entrada de las imágenes de validación. Debe ser múltiplo de 32.\n",
        "* *batch_size* y *learning_rate*: Tamaño de cada *batch* y *learning rate* inicial del optimizador Adam.\n",
        "* *nb_epochs* y *warmup_epochs*: establecen el número de épocas de entrenamiento normales y el número de épocas de entrenamiento iniciales en las que se fuerza a que las *bounding boxes* coincidan con los *anchors*. En total se entranará durante un número de épocas igual a la suma de estos parámetros.\n",
        "* *saved_weights_name*: indica la ruta en la que se guardará el modelo entrenado en formato HDF5.\n",
        "\n",
        "Disponemos opcionalmente de un *callback* de *early stopping* que detiene el entrenamiento si la función de pérdida no mejora en 7 épocas. También tenemos otro *callback* (activado por defecto) que reduce el *learning rate* en un orden de magnitud si la pérdida no mejora en 2 épocas.\n",
        "\n",
        "El archivo con el modelo entrenado puede usarse para renaudar el entrenamiento (cargándolo con `load_weights`) o para realizar predicciones (cargándolo con `load_model`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmH8M3ZZzGoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "from keras.models import load_model\n",
        "from yolo.train import create_training_instances, create_model, create_callbacks\n",
        "from yolo.generator import BatchGenerator\n",
        "from yolo.utils.utils import normalize\n",
        "\n",
        "def _train(epochs, fine_tune = 0, early_stop = False):\n",
        "    ###############################\n",
        "    #   Leemos anotaciones\n",
        "    ###############################\n",
        "    train_ints, _, labels, max_box_per_image = create_training_instances(\n",
        "        config['train']['train_annot_folder'],\n",
        "        config['train']['train_image_folder'],\n",
        "        config['train']['cache_name'],\n",
        "        config['valid']['valid_annot_folder'],\n",
        "        config['valid']['valid_image_folder'],\n",
        "        config['valid']['cache_name'],\n",
        "        config['model']['labels']\n",
        "    )\n",
        "    print('\\nTraining on: \\t' + str(labels) + '\\n')\n",
        "\n",
        "    ##################################\n",
        "    #   Creamos generador de imágenes\n",
        "    ##################################\n",
        "    train_generator = BatchGenerator(\n",
        "        instances           = train_ints,\n",
        "        anchors             = config['model']['anchors'],\n",
        "        labels              = labels,\n",
        "        downsample          = 32, # ratio between input and output size\n",
        "        max_box_per_image   = max_box_per_image,\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        min_net_size        = config['model']['min_input_size'],\n",
        "        max_net_size        = config['model']['max_input_size'],\n",
        "        shuffle             = True,\n",
        "        jitter              = 0.3,\n",
        "        norm                = normalize\n",
        "    )\n",
        "\n",
        "    ###############################\n",
        "    #   Creamos el modelo\n",
        "    ###############################\n",
        "    if os.path.exists(config['train']['saved_weights_name']):\n",
        "        config['train']['warmup_epochs'] = 0\n",
        "    warmup_batches = config['train']['warmup_epochs'] * (config['train']['train_times']*len(train_generator))\n",
        "\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = config['train']['gpus']\n",
        "    multi_gpu = len(config['train']['gpus'].split(','))\n",
        "\n",
        "    train_model, infer_model = create_model(\n",
        "        nb_class            = len(labels),\n",
        "        anchors             = config['model']['anchors'],\n",
        "        max_box_per_image   = max_box_per_image,\n",
        "        max_grid            = [config['model']['max_input_size'], config['model']['max_input_size']],\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        warmup_batches      = warmup_batches,\n",
        "        ignore_thresh       = config['train']['ignore_thresh'],\n",
        "        multi_gpu           = multi_gpu,\n",
        "        saved_weights_name  = config['train']['saved_weights_name'],\n",
        "        lr                  = config['train']['learning_rate'],\n",
        "        grid_scales         = config['train']['grid_scales'],\n",
        "        obj_scale           = config['train']['obj_scale'],\n",
        "        noobj_scale         = config['train']['noobj_scale'],\n",
        "        xywh_scale          = config['train']['xywh_scale'],\n",
        "        class_scale         = config['train']['class_scale'],\n",
        "        backend_path        = DIR + \"models/backend.h5\",\n",
        "        fine_tune           = fine_tune\n",
        "    )\n",
        "\n",
        "    ################################\n",
        "    #   Comenzamos el entrenamiento\n",
        "    ################################\n",
        "    callbacks = create_callbacks(config['train']['saved_weights_name'], config['train']['tensorboard_dir'], infer_model)\n",
        "\n",
        "    if not early_stop:\n",
        "      callbacks = callbacks[1:]\n",
        "\n",
        "    hist = train_model.fit_generator(\n",
        "        generator        = train_generator,\n",
        "        steps_per_epoch  = len(train_generator) * config['train']['train_times'],\n",
        "        epochs           = epochs + config['train']['warmup_epochs'],\n",
        "        verbose          = 2 if config['train']['debug'] else 1,\n",
        "        callbacks        = callbacks,\n",
        "        workers          = 4,\n",
        "        max_queue_size   = 8\n",
        "    )\n",
        "\n",
        "    model_loaded = False\n",
        "    print(\"\\nTraining completed.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtaPXkCWi89",
        "colab_type": "text"
      },
      "source": [
        "## Evaluación\n",
        "\n",
        "Evaluamos el modelo obtenido en el conjunto de validación. La métrica de evaluación utilizada es la precisión media o *mAP* tal y como se evalúa en el [*COCO Challenge*](http://cocodataset.org/#detection-eval) desde 2017. La media se toma variando en 10 umbrales distintos de IoU: 0.5, 0.6, 0.65, ..., 0.95. Tras finalizar, se guardan las *bounding boxes* detectadas para cada imagen en el archivo definido en la variable *valid_result* del archivo de configuración. Se utiliza el formato definido [aquí](https://competitions.codalab.org/competitions/20146#learn_the_details-overview). Es posible crear únicamente el archivo de *valid_result* sin calcular las métricas de precisión."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBV0We-91w3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yolo.evaluate import evaluate_coco, predict_boxes\n",
        "from yolo.generator import BatchGenerator\n",
        "from yolo.voc import parse_voc_annotation\n",
        "from yolo.utils.utils import normalize\n",
        "from keras.models import load_model\n",
        "\n",
        "def _evaluate(model, calculate_ap = False):\n",
        "    # Leemos las anotaciones\n",
        "    valid_ints, labels = parse_voc_annotation(\n",
        "        config['valid']['valid_annot_folder'],\n",
        "        config['valid']['valid_image_folder'],\n",
        "        config['valid']['cache_name'],\n",
        "        config['model']['labels']\n",
        "    )\n",
        "\n",
        "    labels = labels.keys() if len(config['model']['labels']) == 0 else config['model']['labels']\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    # Creamos el generador de imágenes de validación\n",
        "    valid_generator = BatchGenerator(\n",
        "        instances           = valid_ints,\n",
        "        anchors             = config['model']['anchors'],\n",
        "        labels              = labels,\n",
        "        downsample          = 32,\n",
        "        max_box_per_image   = 0,\n",
        "        batch_size          = config['train']['batch_size'],\n",
        "        min_net_size        = config['model']['min_input_size'],\n",
        "        max_net_size        = config['model']['max_input_size'],\n",
        "        shuffle             = True,\n",
        "        jitter              = 0.0,\n",
        "        norm                = normalize\n",
        "    )\n",
        "\n",
        "    # Realizamos predicciones y guardamos resultados en fichero\n",
        "    detections, annotations = predict_boxes(\n",
        "        model,\n",
        "        valid_generator,\n",
        "        obj_thresh = 0.5,\n",
        "        nms_thresh = 0.45,\n",
        "        net_h=config['model']['input_size'],\n",
        "        net_w=config['model']['input_size'],\n",
        "        save_path = config['valid']['valid_result'])\n",
        "\n",
        "    if calculate_ap:\n",
        "        # Calculamos las métricas de evaluación   \n",
        "        m_ap, ap = evaluate_coco(\n",
        "            model,\n",
        "            valid_generator,\n",
        "            detections,\n",
        "            annotations)\n",
        "\n",
        "        # Imprimimos el resultado\n",
        "        print('mAP@.5:.05:.95: {:.4f}'.format(m_ap[0]))\n",
        "        print('AP@0.5: {:.4f}'.format(ap[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGlwF8b6s_oD",
        "colab_type": "text"
      },
      "source": [
        "## Detección\n",
        "\n",
        "Definimos funciones para utilizar nuestro modelo como detector de caras tanto en imágenes como en vídeos. \n",
        "\n",
        "En el primer caso se muestra la imagen con las caras detectadas, y opcionalmente se muestra la posición real de las caras según las anotaciones de *ground truth*. Recibe como entrada un archivo que puede ser una anotación (si se quiere mostrar el valor real de las *bounding boxes* anotadas) o la ruta de una imagen.\n",
        "\n",
        "En el segundo caso, partiendo de un vídeo de entrada se construye un nuevo vídeo donde aparecen las *bounding boxes* detectadas en cada frame, de forma continua."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee2Z0UNPtAFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from yolo.utils.bbox import draw_boxes, BoundBox\n",
        "from keras.models import load_model\n",
        "from yolo.utils.utils import get_yolo_boxes\n",
        "from yolo.voc import parse_single_annot\n",
        "from tqdm import tqdm\n",
        "\n",
        "def _detect_one(\n",
        "    model, \n",
        "    filein, \n",
        "    img_dir, \n",
        "    is_annot = False,\n",
        "    show = True, \n",
        "    show_ground_truth = False):\n",
        "    if is_annot:\n",
        "        # Analizamos la anotación\n",
        "        im_inst = parse_single_annot(filein, img_dir)\n",
        "        filename = im_inst['filename'] \n",
        "    else:\n",
        "        filename = filein\n",
        "\n",
        "    # Leemos la imagen\n",
        "    im = read_im(filename)\n",
        "\n",
        "    # Ground truth\n",
        "    if show_ground_truth:\n",
        "        ground_boxes = [BoundBox(obj['xmin'], obj['ymin'], obj['xmax'], obj['ymax']) \n",
        "                           for obj in im_inst['object']]\n",
        "\n",
        "    # Predecimos las bounding boxes     \n",
        "    pred_boxes = get_yolo_boxes(\n",
        "        model, \n",
        "        images = [im],\n",
        "        net_h = config['model']['input_size'], \n",
        "        net_w = config['model']['input_size'], \n",
        "        anchors = config['model']['anchors'],\n",
        "        obj_thresh = 0.5, \n",
        "        nms_thresh = 0.45)[0]\n",
        "  \n",
        "    # Dibujamos las bounding boxes\n",
        "    im_boxes = draw_boxes(im, pred_boxes, color = (0, 255, 25))\n",
        "\n",
        "    if show_ground_truth:\n",
        "        im_boxes = draw_boxes(im, ground_boxes, show_score = False, color = (255, 0, 25))\n",
        "\n",
        "    # Mostramos la imagen\n",
        "    if show:\n",
        "        print_im(im_boxes)\n",
        "\n",
        "    return im_boxes\n",
        "\n",
        "def _detect_video(model, filein, fileout):\n",
        "    # Abrimos el vídeo\n",
        "    video_reader = cv2.VideoCapture(filein)\n",
        "\n",
        "    # Recolectamos información del vídeo\n",
        "    frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = video_reader.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Creamos vídeo de salida\n",
        "    video_writer = cv2.VideoWriter(\n",
        "        fileout,\n",
        "        cv2.VideoWriter_fourcc(*'MPEG'), \n",
        "        fps, \n",
        "        (frame_w, frame_h))\n",
        "    \n",
        "    batch_size  = 1\n",
        "    images      = []\n",
        "    for i in tqdm(range(frames)):\n",
        "        # Leemos frames del vídeo\n",
        "        _, image = video_reader.read()\n",
        "        images += [image]\n",
        "\n",
        "        if (i%batch_size == 0) or (i == (frames-1) and len(images) > 0):\n",
        "            # Predecimos bounding boxes\n",
        "            batch_boxes = get_yolo_boxes(\n",
        "                model, \n",
        "                images, \n",
        "                config['model']['input_size'], \n",
        "                config['model']['input_size'], \n",
        "                config['model']['anchors'], \n",
        "                obj_thresh = 0.6, \n",
        "                nms_thresh = 0.4)\n",
        "\n",
        "            for i in range(len(images)):\n",
        "                # Dibujamos bounding boxes\n",
        "                draw_boxes(images[i], batch_boxes[i], show_score = False)   \n",
        "\n",
        "                # Escribimos imágenes en el vídeo de salida\n",
        "                video_writer.write(images[i]) \n",
        "\n",
        "        images = []\n",
        "\n",
        "    # Liberamos recursos\n",
        "    video_reader.release()\n",
        "    video_writer.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36PHlVlCROP5",
        "colab_type": "text"
      },
      "source": [
        "## Prueba de entrenamiento y evaluación\n",
        "\n",
        "Probamos ahora a entrenar y evaluar el modelo con la configuración dada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e-ym4bZWgwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos los parámetros de entrenamiento\n",
        "EARLY_STOP = True\n",
        "FINE_TUNE = 0\n",
        "EPOCHS = config['train']['nb_epochs']\n",
        "\n",
        "# Entrenamos el modelo\n",
        "_train(EPOCHS, FINE_TUNE, EARLY_STOP)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqFMlnx1bUK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cargamos el modelo\n",
        "if not model_loaded:\n",
        "    yolov3 = load_model(config['valid']['valid_model'])\n",
        "    print(\"Cargado modelo \" + config['valid']['valid_model'])\n",
        "    model_loaded = True\n",
        "\n",
        "print(config['valid']['valid_model'])\n",
        "# Evaluamos el modelo\n",
        "_evaluate(yolov3, True)\n",
        "print(\"\\nEvaluation finished.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvk5msRd1X4V",
        "colab_type": "text"
      },
      "source": [
        "## Prueba de detección\n",
        "\n",
        "Detectamos ahora caras en una imagen (con y sin las *bounding boxes* verdaderas) y probamos a realizar detección en vídeo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6mIJnZbcBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cargamos el modelo\n",
        "if not model_loaded:\n",
        "    yolov3 = load_model(config['valid']['valid_model'])\n",
        "    print(\"Cargado modelo \" + config['valid']['valid_model'])\n",
        "    model_loaded = True\n",
        "\n",
        "# Definimos un par de imágenes de prueba\n",
        "annot1 = config['valid']['valid_annot_folder'] \\\n",
        "    + \"9_Press_Conference_Press_Conference_9_74.xml\"\n",
        "annot2 = config['valid']['valid_annot_folder'] \\\n",
        "    + \"9_Press_Conference_Press_Conference_9_89.xml\"\n",
        "\n",
        "# Detectamos caras en las imágenes de prueba\n",
        "_ = _detect_one(\n",
        "    yolov3, \n",
        "    annot1, \n",
        "    config['valid']['valid_image_folder'], \n",
        "    is_annot = True, \n",
        "    show_ground_truth = True)\n",
        "_ = _detect_one(\n",
        "    yolov3, \n",
        "    annot2, \n",
        "    config['valid']['valid_image_folder'], \n",
        "    is_annot = True, \n",
        "    show_ground_truth = False)\n",
        "\n",
        "# Detectamos caras en un vídeo\n",
        "_detect_video(yolov3, DIR + \"test.mp4\", DIR + \"test_out.mp4\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}